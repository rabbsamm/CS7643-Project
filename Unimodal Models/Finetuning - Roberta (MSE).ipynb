{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2482bce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, RobertaModel, AutoModel, AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from string import digits\n",
    "from html import unescape\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchmetrics.functional.classification import auroc, accuracy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import pytorch_lightning as pl\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accd525c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r'C:\\Users\\rabby\\CS 7643 - Deep Learning\\Project')\n",
    "\n",
    "config = {\n",
    "    'model_name': 'roberta-base',\n",
    "    'n_labels': 2,\n",
    "    'batch_size': 64,\n",
    "    'dropout': 0.2,\n",
    "    'lr': 1e-5,\n",
    "    'n_epochs': 20,\n",
    "    'device': 'cuda',\n",
    "    'n_threads': 1000\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8169b4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pol_path = r'Data\\pol_0616-1119_labeled\\pol_062016-112019_labeled.ndjson'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccac0da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Clean_Post(post):\n",
    "    # Remove HTML tags\n",
    "    cleaned_post = re.sub('<.*?>', '', post)\n",
    "\n",
    "    # Unescape HTML entities\n",
    "    cleaned_post = unescape(cleaned_post)\n",
    "\n",
    "    # Remove line breaks and extra spaces\n",
    "    cleaned_post = cleaned_post.replace('\\n', ' ').replace('\\r', ' ').strip()\n",
    "    \n",
    "    x = cleaned_post.split('>')\n",
    "    cleaned_post = ' '.join(x).strip()\n",
    "    cleaned_post = cleaned_post.lstrip(digits)\n",
    "\n",
    "    return cleaned_post\n",
    "\n",
    "def Load_Pol(pol_path, n_threads = 10**5, thresholds = {'tox': 0.2, 'inf': 0.2}, test_size = 0.2, random_state = 24):\n",
    "    \n",
    "    #Load first n_threads rows from 4chan data\n",
    "    pol_raw = pd.read_json(pol_path, lines = True, nrows = n_threads)\n",
    "    #Declare variables for storage of posts and toxicity scores\n",
    "    posts = []\n",
    "    toxicity = []\n",
    "    inflammatory = []\n",
    "    #Extract posts, scores from nested dictionaries\n",
    "    for i in range(n_threads):\n",
    "        thread = pol_raw.loc[i][0]\n",
    "        n_posts = len(thread)\n",
    "        for j in range(n_posts):\n",
    "            try:\n",
    "                posts.append(thread[j]['com'])\n",
    "                toxicity.append(thread[j]['perspectives']['SEVERE_TOXICITY'])\n",
    "                inflammatory.append(thread[j]['perspectives']['INFLAMMATORY'])\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    #Create pol_data df\n",
    "    pol_data = pd.DataFrame(data = {'Posts': posts, 'Toxicity': toxicity, 'Inflammatory': inflammatory})\n",
    "    #Clean comments\n",
    "    pol_data.loc[:, 'Posts'] = pol_data.loc[:, 'Posts'].apply(Clean_Post)\n",
    "    #Set Toxic Flag to 1 for posts exceeding toxicity and inflammatory thresholds\n",
    "\n",
    "    #Split Train and Val\n",
    "    pol_train, pol_val = train_test_split(pol_data, test_size = test_size, random_state = random_state)\n",
    "    \n",
    "    return pol_train, pol_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca95e98f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c3adee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pol_Dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data, tokenizer, max_length = 128):\n",
    "        #Declare variables\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        #Select index\n",
    "        item = self.data.iloc[index]\n",
    "        #Extract posts and toxicity flag\n",
    "        post = str(item.Posts)\n",
    "        label = torch.Tensor([item[['Toxicity', 'Inflammatory']]])\n",
    "        #Convert to tokens\n",
    "        tokens = self.tokenizer.encode_plus(post, add_special_tokens = True, return_tensors = 'pt', truncation = True, \n",
    "                                           max_length = self.max_length, padding = 'max_length', return_attention_mask = True)\n",
    "        \n",
    "        return {'input_ids': tokens.input_ids.flatten(), 'attention_mask': tokens.attention_mask.flatten(), 'labels': label}\n",
    "    \n",
    "class Pol_Data_Module(pl.LightningDataModule):\n",
    "    \n",
    "    def __init__(self, train, val, test = None, batch_size = 16, max_length = 128,  model = 'roberta-base'):\n",
    "        super().__init__()\n",
    "        self.train = train\n",
    "        self.val = val\n",
    "        if test == None:\n",
    "            self.test = val\n",
    "        else:\n",
    "            self.test = test\n",
    "        self.batch_size = batch_size\n",
    "        self.max_length = max_length\n",
    "        self.model = model\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model)\n",
    "        \n",
    "    def setup(self, stage = None):\n",
    "        self.train_ds = Pol_Dataset(self.train, self.tokenizer, max_length = self.max_length)\n",
    "        self.val_ds = Pol_Dataset(self.val, self.tokenizer, max_length = self.max_length)\n",
    "        self.test_ds = Pol_Dataset(self.test, self.tokenizer, max_length = self.max_length)\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_ds, batch_size = self.batch_size, shuffle = True)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_ds, batch_size = self.batch_size, shuffle = False)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_ds, batch_size = self.batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e51f546",
   "metadata": {},
   "outputs": [],
   "source": [
    "class pol_Classifier(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.model = AutoModel.from_pretrained(config['model_name'], return_dict = True)\n",
    "        self.linear = nn.Sequential(nn.Linear(self.model.config.hidden_size, self.model.config.hidden_size),\n",
    "                                    nn.ReLU(), \n",
    "                                    nn.Dropout(config['dropout']), \n",
    "                                    nn.Linear(self.model.config.hidden_size, self.config['n_labels']))\n",
    "        torch.nn.init.xavier_uniform_(self.linear[0].weight)\n",
    "        torch.nn.init.xavier_uniform_(self.linear[3].weight)\n",
    "        self.softmax = nn.Softmax(dim = 1)\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.training_step_outputs = []\n",
    "        self.validation_step_outputs = []\n",
    "        self.tloss = []\n",
    "        self.vloss = []\n",
    "           \n",
    "    def forward(self, input_ids, attention_mask, labels = None):\n",
    "        out = self.model(input_ids = input_ids, attention_mask = attention_mask)\n",
    "        out = torch.mean(out.last_hidden_state, 1)\n",
    "        # final logits\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "    \n",
    "    def training_step(self, batch, batch_index):\n",
    "        loss, out, y = self._common_step(batch, batch_index)\n",
    "        self.training_step_outputs.append(loss)\n",
    "        self.log(\"Training Loss\", loss, prog_bar = True, logger = True)\n",
    "        return loss\n",
    "    \n",
    "    def on_train_epoch_end(self):\n",
    "        epoch_mean = torch.stack(self.training_step_outputs).mean()\n",
    "        self.tloss.append(float(epoch_mean.detach().cpu().numpy()))\n",
    "        self.training_step_outputs.clear()\n",
    "    \n",
    "    def validation_step(self, batch, batch_index):\n",
    "        loss, out, y = self._common_step(batch, batch_index)\n",
    "        self.validation_step_outputs.append(loss)\n",
    "        self.log(\"Validation Loss\", loss, prog_bar = True, logger = True)\n",
    "        return loss\n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        epoch_mean = torch.stack(self.validation_step_outputs).mean()\n",
    "        self.vloss.append(float(epoch_mean.detach().cpu().numpy()))\n",
    "        self.validation_step_outputs.clear()\n",
    "    \n",
    "    def test_step(self, batch, batch_index):\n",
    "        loss, out, y = self._common_step(batch, batch_index)\n",
    "        self.log(\"Test Loss\", loss, prog_bar = True, logger = True)\n",
    "        return loss\n",
    "\n",
    "    def predict_step(self, batch, batch_index):\n",
    "        loss, out, y = self._common_step(batch, batch_index)\n",
    "        return loss\n",
    "    \n",
    "    def _common_step(self, batch, batch_index):\n",
    "        x = batch['input_ids']\n",
    "        y = batch['labels'].squeeze(1)\n",
    "        attn_mask = batch['attention_mask']\n",
    "        out = self.forward(x, attn_mask)\n",
    "        y = y.to(config['device'])\n",
    "        loss = self.loss(out, y)\n",
    "        return loss, out, y\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr = self.config['lr'])\n",
    "        return [optimizer]\n",
    "    \n",
    "    def plot_loss(self):\n",
    "        self.vloss.pop()\n",
    "        plt.plot(self.tloss, label = 'Training')\n",
    "        plt.plot(self.vloss, label = 'Validation')\n",
    "        plt.title('Loss')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "    def plot_auroc(self):\n",
    "        self.vauroc.pop()\n",
    "        plt.plot(self.tauroc, label = 'Training')\n",
    "        plt.plot(self.vauroc, label = 'Validation')\n",
    "        plt.title('AUROC')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "    def plot_accuracy(self):\n",
    "        self.vacc.pop()\n",
    "        plt.plot(self.tacc, label = 'Training')\n",
    "        plt.plot(self.vacc, label = 'Validation')\n",
    "        plt.title('Accuracy')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7a5045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datamodule\n",
    "pol_train, pol_val = Load_Pol(pol_path, n_threads = config['n_threads'], thresholds = config['thresholds'])\n",
    "pol_data_module = Pol_Data_Module(pol_train, pol_val, batch_size = config['batch_size'])\n",
    "\n",
    "# model\n",
    "model = pol_Classifier(config)\n",
    "\n",
    "# trainer and fit\n",
    "trainer = pl.Trainer(max_epochs = config['n_epochs'], devices = 1, accelerator = \"gpu\", num_sanity_val_steps = 0)\n",
    "trainer.fit(model, pol_data_module)\n",
    "trainer.validate(model, pol_data_module)\n",
    "trainer.test(model, pol_data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f878fd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e5114b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.plot_auroc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd708dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.plot_accuracy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
